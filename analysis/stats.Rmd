---
title: "Learn statistics"
author: "Dave Tang"
date: "`r Sys.Date()`"
output:
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Basics

Statistics are summaries or collections of numbers. The mean, median, and mode are all summary statistics. When we can't take every single possible measurement (the population), which is often the case, we take an estimation using a sample.

Statistics uses a lot of notation (usually algebraic), which is simply a shorthand way of expressing statistical computations. Most statistical calculations simply involve addition, subtraction, multiplication, division, and exponentiation. Common calculations include the "sum of cross products" and "sum of squares":

$$
\frac{(\sum xy)^2}{\sum x^2 + \sum y^2} = \frac{(sum\ of\ cross\ products)^2}{sum\ of\ squares\ for\ x + sum\ of\ squares\ for\ y}
$$

Calculating the mean is simply:

$$
\bar{x} = \frac{\sum x}{n}
$$

## Variance

The mean as a summary statistic is useful but it does not provide any information on the variance. For example, the two datasets below have the same mean but as you can see have different variances.

```{r eg1}
x1 <- c(48, 49, 49, 47, 48, 47)
x2 <- c(12, 62, 3, 50, 93, 68)
mean(x1) == mean(x2)
```

The easiest (but least useful) way to summarise variation is by using the range; it's not that useful because it uses just two values (the lowest and highest values) from the total dataset.

```{r eg1_range}
range(x1)
range(x2)
```

To make the best use of the datasets, we need a statistic that uses all the numbers. One key piece of intuition is to realise that **if all numbers are identical, there would be no variation and the numbers would all equal the mean**.

```{r}
x3 <- rep(48, 6)
mean(x3)
```

Therefore, if the numbers are not the same, each number's contribution to the total variation is its _deviation_ (difference) from the mean. We could add all these differences (ignoring whether the difference is + or -) and come up with a "total deviation": $\sum(x - \bar{x})$.

```{r total_deviation}
sum(abs(x1 - mean(x1)))
sum(abs(x2 - mean(x2)))
```

However the main problem with this method is that we can only compare total deviations between datasets with the same number. We need a deviation statistic that is independent of sample size, in the same way the mean is.

We can divide the total deviation by the total number of numbers to get the mean (average) deviation:

$$
\frac{\sum(x - \bar{x})}{n} = \frac{The\ sum\ of\ all\ the\ deviations\ from\ the\ mean}{The\ number\ of\ numbers}
$$

```{r mean_deviation}
sum(abs(x1 - mean(x1))) / length(x1)
sum(abs(x2 - mean(x2))) / length(x2)
```

The mean average deviation is very close to the standard measure of variation used in statistics, which is the variance.

$$
variance = \frac{\sum(x - \bar{x})^2}{n - 1} = \frac{The\ sum\ of\ all\ the\ squared\ deviations\ from\ the\ mean}{One\ less\ than\ the\ number\ of\ numbers}
$$

```{r variance}
var(x1)
var(x2)
```

The bottom part of the variance equation is known as the _degrees of freedom_ (d.f.) and the top part, $\sum(x - \bar{x})^2$ is known as the _sum of squares_ but should really be called the _sum of squares of deviations_ since that is what we are calculating.

It is essential to remember that:

* _Sum of squares_ in statistics is the technical term for _summed squared deviations from the mean_.

## Why n - 1?

The two important basics of degrees of freedom are:

1. We are calculating statistics from just a small sample rather than the entire population of numbers and
2. The mean that is used as the basis for the deviations that contribute to the sum of squares is based on the **total** of just that small sample.

In ignorance of the true mean of the population, we are forced into using the total of our sample to calculate a mean from which the deviations are then calculated. It is that use of the **total** which restricts our freedom from $n$ to $n - 1$. If we used the true mean of the population and not the mean of the sample in calculating the sum of squares, we would divide by $n$ instead.

As an analogy imagine a bookshelf and that we are trying to work out the mean thickness of all the books. We take a small sample of six books. and their combined thickness is 158 mm giving a mean of 26.3 mm. We need to know the individual deviation from the mean for each of the six books to calculate the variance.

We measure the thickness of one book (1 d.f.) and it is 22mm. The remaining five books must total $158 - 22 = 136mm$ in thickness. We measure one more book (2 d.f.) and it is 24 mm thick. By the time, we measured the fifth book (fifth degree of freedom), the combined thickness is 129 mm. **There are no more degrees of freedom for the six books!** We do not need to pick up and measure the last book; it must be $158 - 129 = 29mm$. Therefore, given the mean of the sample, we know **the total thickness of the six books** and from this the individual thicknesses of all six books after measuring only five (hence 5 d.f. for a sample of six!).

By using the sample mean as the base from which to calculate the deviations for the sum of squares, we have lost 1 d.f. Thus variance is not the simple mean of the squared deviations, it is the squared deviation per **opportunity for variation** once we have been given the sample mean.

## Why are the deviations squared?

[You might](https://math.stackexchange.com/a/937602) look at your data as measured in a multidimensional space, where each subject is a dimension and each item is a vector in that space from the origin towards the items' measurement over the full subject's space.

Additional remark: this view of things has an additional nice flavour because it uncovers the condition, that the subjects are assumed independent of each other. This is to have the data-space Euclidean; changes in that independence-condition require then changes in the mathematics of the space: it has correlated (or "oblique") axes.

Now the distance of one vector-arrowhead to another is just the formula for distances in the Euclidean space, the square root of squares of distances-of-coordinates (from the Pythagorean theorem):

$$
d = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \cdots + (x_n - y_n)^2}
$$

And the standard deviation is that value, normalised by the number of subjects, if the mean-vector is taken as the y-vector.

$$
s = \sqrt{\frac{(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + \cdots + (x_n - \bar{x})^2}{n}}
$$

## The standard deviation

Standard deviation is the square root of variance.

```{r sd}
sd(x1) == sqrt(var(x1))
sd(x2)
```

In the case of `x2` the mean and standard deviation is $48.0 \pm 34.5$, which suggests variation between $48.0-34.5=13.5$ and $48.0+34.5=82.5$.

The standard deviation was once called _root mean square deviation_. This was a helpful name as it reminds us of the calculation in reverse (deviation, square, mean, root), where mean is $n -1$.

$$
s = \sqrt{\frac{\sum (x - \bar{x})^2}{n - 1}}
$$

## The normal distribution

The word _distribution_ is short for _frequency distribution_, i.e., the frequency with which different numbers are found in a population. The normal distribution is a particular pattern of variation of numbers around the mean. It is symmetrical with the frequency of individual numbers falling off equally away from the mean in both directions.

[One argument (and demonstration)](https://ekamperi.github.io/mathematics/2021/01/29/why-is-normal-distribution-so-ubiquitous.html) for why normal distributions are so common:

> because many variables in the real world are the sum of other independent variables. And, when independent variables are added together, their sum converges to a normal distribution.

Read in example dataset containing the height (inches) and weights (pounds) of 25,000 different humans of 18 years of age (that are converted back to cms and kgs).

```{r}
readr::read_csv(
  file = "data/SOCR-HeightWeight.csv.gz",
  show_col_types = FALSE,
  col_names = c('index', 'height', 'weight'),
  skip = 1
) |>
  dplyr::mutate(height = height * 2.54) |>
  dplyr::mutate(weight = weight / 2.205) -> socr

tail(socr)
```

Plot distribution of heights.

```{r}
library(ggplot2)
ggplot(socr, aes(height)) +
  geom_histogram(
    aes(y = after_stat(density)),
    bins = 30,
    colour = "#000000",
    fill = "#FFFFFF"
  ) +
  geom_density() +
  geom_vline(
    xintercept = mean(socr$height),
    colour = "#990000",
    lty = 2
  ) +
  theme_minimal() +
  ggtitle("Distribution of heights in 25,000 18 year olds") +
  NULL
```

We can see that the mean height coincides with the most frequent height and that the heights are symmetrical about the mean.

The mean and standard deviation is $172.70 \pm 4.83$.

```{r socr_mean_sd}
mean(socr$height)
sd(socr$height)
```

In a normal distribution, 68% of the data lie between the mean $\pm$ 1 $s$, which is what we observe in this dataset.

```{r}
x <- mean(socr$height)
s <- sd(socr$height)

prop.table(table(socr$height > x - s & socr$height < x + s))
```

95% of the data lie between the mean $\pm$ 2 $s$, which is true again in this dataset.

```{r}
x <- mean(socr$height)
s <- sd(socr$height)

prop.table(table(socr$height > x - 2 * s & socr$height < x + 2 * s))
```

### Checking for normality

>[In statistics](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot), a Q–Q plot (quantile–quantile plot) is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other.

The `qqnorm` function produces a normal Q-Q plot (where the x-axis contains the theoretical quantiles of a normal distribution); the `qqline` function adds a line to the "theoretical" Q-Q plot. Our height data fits the theoretical line almost perfectly.

```{r qqnorm}
qqnorm(socr$height, pch = 16)
qqline(socr$height, col = 2, lty = 2)
```

However, biological data is typically not normally distributed but are asymmetrical. We often find data peak well to the left with a long right tail. The mean of the distribution will not coincide with the peak of the data and therefore using the normal distribution is clearly not appropriate.

Another important point is our estimate of the standard deviation, which is based on a sample of the population. If our sample size is too small, our estimate of the standard deviation can be very inaccurate and therefore our estimate of the distribution.

If a distribution is non-normal, a function such as logarithm or square root can be used to normalise the distribution; this is known as [transformation](https://www.biostathandbook.com/transformation.html).

To illustrate transformation we will [use data](https://www.biostathandbook.com/transformation.html) showing the abundance of the fish species _Umbra pygmaea_ (Eastern mudminnow) in Maryland streams, which is non-normally distributed; there are a lot of streams with a small density of mudminnows, and a few streams with lots of them. Below are 12 numbers from the mudminnow data set.

```{r mudminnow}
library(ggridges)
library(ggplot2)

tibble::tibble(
  raw = c(38, 1, 13, 2, 13, 20, 50, 9, 28, 6, 4, 43)
) |>
  dplyr::mutate(stream = dplyr::row_number()) |>
  dplyr::mutate(sqrt = sqrt(raw)) |>
  dplyr::mutate(log = log(raw)) |>
  dplyr::mutate(reciprocal = 1/raw) |>
  dplyr::mutate(arcsine = asin(sqrt(raw/max(raw)))) |>
  dplyr::select(stream, dplyr::everything()) |>
  tidyr::pivot_longer(-stream, names_to = 'transformation') |>
  dplyr::mutate(transformation = factor(transformation, levels = c('log', 'reciprocal', 'arcsine', 'sqrt', 'raw'))) -> mudminnow

ggplot(mudminnow, aes(x = value, y = transformation, fill = transformation)) +
  geom_density_ridges(alpha=0.6, stat="binline", bins=30) +
  theme_ridges() +
  NULL
```

A square root transformation is used for data that are clumped and the variance is greater than the mean. As there is no square root for negative numbers, a constant needs to be added to each observation to make all observations positive. Note that the square root of a fraction becomes larger than the observation being transformed.

```{r sqrt_larger_than_orig}
x <- 0.16
sqrt(x) > x
```

A reciprocal transformation is used when clumping of the data is very high. Since the reciprocal of $x$ is $\frac{1}{x}$, the transformation leads to large numbers being smaller after transformation than smaller ones: $\frac{1}{2} \gt \frac{1}{10}$.

An arcsine transformation can be used to "stretch out" data points that range between the values 0 and 1. This type of transformation is typically used when dealing with proportions and percentages, especially where high or low percentages are involved. However, it makes little difference in the 30-70% range. In the code above, the raw counts were divided by the maximum value to create percentage-like values.

```{r arcsine}
mudminnow |>
  dplyr::filter(transformation == "raw") |>
  dplyr::pull(value) -> x

x / max(x)
asin(sqrt(x / max(x)))
```

Data is transformed to make it suitable for statistical analysis and subsequent statistical comparisons.

## Visualise

[Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet).

```{r anscombe}
eg1 <- datasets::anscombe
```






